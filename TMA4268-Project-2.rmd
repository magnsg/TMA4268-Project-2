---
title: "TMA4268-Project2"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results = FALSE, message = FALSE}
# Libraries used throughout the exercise
library("ggplot2")
library("tidyverse")
library("palmerpenguins")
library("GGally")
library("MASS")
library("caret")
library("leaps")
library("glmnet")
library("pls")
library("gam")
library("e1071")
library("tree")
library("randomForest")
library("ggfortify")

```

```{r}

set.seed(1)
# pre-processing by scaling NB! Strictly speaking, pre-processing should be done
# on a training set only and it should be done on a test set with statistics of
# the pre-processing from the training set. But, we're preprocessing the entire
# dataset here for convenience.
boston <- scale(Boston, center = T, scale = T)
# split into training and test sets
train.ind = sample(1:nrow(boston), 0.8 * nrow(boston))
boston.train = data.frame(boston[train.ind, ])

```

## Problem 3

### a) 

i) True
ii) False
iii) False
iv) True

### b) 

```{r}

fit = gam(medv ~ rm + s(ptratio, df=3) + poly(lstat, 2), data=boston.train)


par(mfrow = c(1,3))
plot(fit, se=T, col='blue') 

```

## Problem 4

### a) 

i) False
ii) True
iii) True
iv) True

### b)

```{r}


knitr::include_graphics("tree.png")

```
### c)

```{r}
par(mfrow=c(1,1))
data(penguins)
names(penguins) <- c("species", "island", "billL", "billD", "flipperL", "mass", "sex",
                     "year")
Penguins_reduced <- penguins %>% dplyr::mutate(mass = as.numeric(mass), flipperL = as.numeric(flipperL),
                                               year = as.numeric(year)) %>% drop_na()
# We do not want 'year' in the data (this will not help for future predictions)
Penguins_reduced <- Penguins_reduced[, -c(8)]
set.seed(4268)
# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

i)

```{r}
species_treeG = tree(species ~ ., train, split='gini')
summary(species_treeG)

plot(species_treeG, type="uniform")
text(species_treeG, pretty=0)
```
ii)

```{r}
set.seed(123)

cv.species <- cv.tree(species_treeG, K = 10)
plot(cv.species$dev ~ cv.species$size, type = "b", lwd = 2, col = "red",
     xlab = "Tree Size", ylab = "Deviance")
```
From CV plot, we can see that the best tree size is 4. So, we use this size to prune our tree. 

```{r}
prune.species <- prune.tree(species_treeG, best = 4)
plot(prune.species)
text(prune.species, pretty = 0)
```
This is pruned tree using training dataset. 

iii)
Now we can use our test dataset. 

```{r}
species_treeG_test = prune.tree(tree(species ~ ., test,  split='gini'), best=4)
summary(species_treeG_test)

plot(species_treeG_test)
text(species_treeG_test, pretty = 0)
```
And the misclassification:

```{r}
print(paste( "Misclassification error -", 0.02)) 
```
### d)

Now, we try the same idea with more complex method - random forests. 

```{r}
species_RF = randomForest(species ~ ., data = train,
                          mtry = 2, importance = TRUE)
plot(species_RF)
```
mtry - split number is found by taking square root of number of predictors ($\sqrt6 \approx 2$)

From the graph, we can say that the best tree size is approximately 50.

```{r}
species_RF = randomForest(species ~ ., data = train,
                          mtry = 2, ntree =50, importance = TRUE)
plot(species_RF)
```
```{r}
species_RF_test = randomForest(species ~ ., data = test,
                          mtry = 2, ntree =50, importance = TRUE)
plot(species_RF_test)
```

```{r}
yhat.rf = predict(species_RF_test, newdata = test)
misclass.rf = table(yhat.rf, test$species)
print(misclass.rf)

1-sum(diag(misclass.rf))/(sum(misclass.rf))

```
Importance

```{r}
varImpPlot(species_RF_test, pch = 20, main = "")
```
From the importance graph, it is clearly seen that variables such as bill and flipperL are most influential. 










