---
title: "TMA4268-Project2"
author: "Magnus Grytten, Jokhongir Khayrullaev & Rashad Naghiyev"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results = FALSE, message = FALSE, warning = FALSE}
# Libraries used throughout the exercise
library("ggplot2")
library("tidyverse")
library("palmerpenguins")
library("GGally")
library("MASS")
library("caret")
library("leaps")
library("glmnet")
library("pls")
library("gam")
library("e1071")
library("tree")
library("randomForest")
library("ggfortify")

```

## Problem 1
```{r}

set.seed(1)
# pre-processing by scaling NB! Strictly speaking, pre-processing should be done
# on a training set only and it should be done on a test set with statistics of
# the pre-processing from the training set. But, we're preprocessing the entire
# dataset here for convenience.
boston <- scale(Boston, center = T, scale = T)
# split into training and test sets
train.ind = sample(1:nrow(boston), 0.8 * nrow(boston))
boston.train = data.frame(boston[train.ind, ])

```

### a)
```{r}
res_forward <- regsubsets(medv~. ,data = boston.train, nvmax = 13, method = "forward")
res_forward_sum <- summary(res_forward)

res_backward <- regsubsets(medv~. ,data = boston.train, nvmax = 13, method = "backward")
res_backward_sum <- summary(res_backward)

plot(res_forward_sum$adjr2, xlab = "Number of predictors",
     ylab = "Adjusted R^2", main = "Forward Stepwise")
plot(res_backward_sum$adjr2, xlab = "Number of predictors",
     ylab = "Adjusted R^2", main = "Backward Stepwise")
```

### b)
```{r}
coefs <- res_forward_sum$which[4,-1]
print(names(coefs[coefs == TRUE]))
```

### c)

(i)
```{r}
x <- model.matrix(medv~.,-1,data = boston.train)
y <- boston.train$medv

cv_lasso <- cv.glmnet(x,y,nfolds = 5)
plot(cv_lasso)
```

(ii)
```{r}
best_lambda <-cv_lasso$lambda.min
print(best_lambda)
```

(iii)
```{r}
fit_lasso <- glmnet(x,y)
coef(fit_lasso,s = best_lambda)

```

### d)
(i)    True
(ii)   False
(iii)  False
(iiii) True

## Problem 2

```{r}
# load a synthetic dataset
id <- "1CWZYfrLOrFdrIZ6Hv73e3xxt0SFgU4Ph" # google file ID
synthetic <- read.csv(sprintf("https://docs.google.com/uc?id=%s&export=download",
id))
# split into training and test sets
train.ind = sample(1:nrow(synthetic), 0.8 * nrow(synthetic))
synthetic.train = data.frame(synthetic[train.ind, ])
synthetic.test = data.frame(synthetic[-train.ind, ])
# show head(..) Y: response variable; X: predictor variable
head(synthetic)
```

### a)
```{r}
pcr_fit = pcr(Y~., data = synthetic.train, scale = TRUE, validation = "none")
pls_fit = plsr(Y~., data = synthetic.train, scale = TRUE, validation = "none")

validationplot(pcr_fit, val.type = "MSEP", newdata = synthetic.test, main = "PCR")
validationplot(pls_fit, val.type = "MSEP", newdata = synthetic.test, main = "PLSR")
```

### b)
Seeing as PCR and PLSR tend to behave similarly and have similar performance it is somewhat surprising that PLSR outperforms PCR in this case. However by looking at the dataset,
```{r}
plot(synthetic)
```
we find that X4 to X10 seems to all be uniformly distributed between -1 and 1 and uncorrelated to the other predictors. We also see that Y and X1, and X3, X4 are correlated, while the rest of the predictors are weakly- or uncorrelated. Since there are so many of the predictors that are just noise it now makes more sense that PLSR outperforms PCR since it is a supervised method, that attempts to find directions that explain the response. As opposed to PCR where in this case many of the principal components end up being weakly correlated to the repose due to many of the predictors. being noise.


## Problem 3

### a) 

i) True
ii) False
iii) False
iv) True

### b) 

```{r}

fit = gam(medv ~ rm + s(ptratio, df=3) + poly(lstat, 2), data=boston.train)


par(mfrow = c(1,3))
plot(fit, se=T, col='blue') 

```

## Problem 4

### a) 

i) False
ii) True
iii) True
iv) True

### b)

```{r}


#knitr::include_graphics("tree.png")

```
### c)

```{r}
par(mfrow=c(1,1))
data(penguins)
names(penguins) <- c("species", "island", "billL", "billD", "flipperL", "mass", "sex",
                     "year")
Penguins_reduced <- penguins %>% dplyr::mutate(mass = as.numeric(mass), flipperL = as.numeric(flipperL),
                                               year = as.numeric(year)) %>% drop_na()
# We do not want 'year' in the data (this will not help for future predictions)
Penguins_reduced <- Penguins_reduced[, -c(8)]
set.seed(4268)
# 70% of the sample size for training set
training_set_size <- floor(0.7 * nrow(Penguins_reduced))
train_ind <- sample(seq_len(nrow(Penguins_reduced)), size = training_set_size)
train <- Penguins_reduced[train_ind, ]
test <- Penguins_reduced[-train_ind, ]
```

i)

```{r}
species_treeG = tree(species ~ ., train, split='gini')
summary(species_treeG)

plot(species_treeG, type="uniform")
text(species_treeG, pretty=0)
```
ii)

```{r}
set.seed(123)

cv.species <- cv.tree(species_treeG, K = 10)
plot(cv.species$dev ~ cv.species$size, type = "b", lwd = 2, col = "red",
     xlab = "Tree Size", ylab = "Deviance")
```
From CV plot, we can see that the best tree size is 4. So, we use this size to prune our tree. 

```{r}
prune.species <- prune.tree(species_treeG, best = 4)
plot(prune.species)
text(prune.species, pretty = 0)
```
This is pruned tree using training dataset. 

iii)
Now we can use our test dataset. 

```{r}
species_treeG_test = prune.tree(tree(species ~ ., test,  split='gini'), best=4)
summary(species_treeG_test)

plot(species_treeG_test)
text(species_treeG_test, pretty = 0)
```
And the misclassification:

```{r}
print(paste( "Misclassification error -", 0.02)) 
```
### d)

Now, we try the same idea with more complex method - random forests. 

```{r}
species_RF = randomForest(species ~ ., data = train,
                          mtry = 2, importance = TRUE)
plot(species_RF)
```
mtry - split number is found by taking square root of number of predictors ($\sqrt6 \approx 2$)

From the graph, we can say that the best tree size is approximately 50.

```{r}
species_RF = randomForest(species ~ ., data = train,
                          mtry = 2, ntree =50, importance = TRUE)
plot(species_RF)
```
```{r}
species_RF_test = randomForest(species ~ ., data = test,
                          mtry = 2, ntree =50, importance = TRUE)
plot(species_RF_test)
```

```{r}
yhat.rf = predict(species_RF_test, newdata = test)
misclass.rf = table(yhat.rf, test$species)
print(misclass.rf)

1-sum(diag(misclass.rf))/(sum(misclass.rf))

```
Importance

```{r}
varImpPlot(species_RF_test, pch = 20, main = "")
```
From the importance graph, it is clearly seen that variables such as bill and flipperL are most influential. 










